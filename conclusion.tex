\section{Conclusion}
This paper reviews the current state-of-the-art of Distributed Machine Learning. To answer this main research question, all sub-research questions are answered:
\begin{itemize}
	\item \textbf{What are the advantages of Distributed Machine Learning over (centralized) Machine Learning?}
	\begin{itemize}
		\item \textbf{Which alternatives exist to Distributed Machine Learning?} We discussed several ways to learn from huge amounts of data, including the use of specialized hardware (such as ASICs) and increased computation power per computer (as seen in e.g. the Epiphany architecture)
		\item \textbf{How likely are they to remain sufficient for emerging applications?} While Distributed Machine Learning will provide the highest amount of power in the future because it scales in the best case almost horizontally, alternatives may remain in use for simpler tasks that are too heavy to run on an ordinary machine, but too lightweight for a full Distributed Machine Learning cluster.
	\end{itemize}
	\item \textbf{What technology is used for Distributed Machine Learning?} We created an overview of the algorithms that a general Distributed Machine Learning could use and how they relate to each other:
	\begin{itemize}
		\item \textbf{What algorithms and with which parameters are used to perform the actual Machine Learning?} We divided ML algorithms into categories based on the type of feedback given to the algorithm, its goal, its type and its method.
		\begin{itemize}
			\item \textbf{What algorithms are used to find the best parameters for the algorithms?} We listed several "workhorse" algorithms that optimize the parameters for ML algorithms, categorized as first-order techniques, second-order techniques, coordinate descent, Markov-Chain Monte Carlo, and Variational inference.
			\item \textbf{What algorithms are used to perform the Machine Learning?} For every category, this paper lists several ML methods, with the notable mention of Artificial Neural Networks (ANNs) and Ensemble Learning.
		\end{itemize}
		\item \textbf{How can these algorithms be partitioned and distributed across a wide variety of hardware?} To partition and distribute Machine Learning, there are 5 sub-research questions that need to be answered:
		\begin{itemize}
			\item \textbf{What is the tradeoff between computation time VS communication VS accuracy?} TODO
			\item \textbf{How to schedule and balance the workloads?} When scheduling and balancing the workloads, there are 3 major considerations: which tasks to execute in parallel, what order to execute the tasks in, and how to evenly distribute the workload across the machines.
			\item \textbf{How to bridge computation and communication?} There's a trade-off between correct model convergence and faster updates. BSP and SSP focus on correct model convergence, but have a lot of communication overhead because of synchronization barriers. ASP is very communication-efficient, but parameter selection is complicated. BAP / TAP are also very communication-efficient, but provide slow or even incorrect model convergence.
			\item \textbf{What strategies can be used to optimize communication overhead?} To prevent bursts of communication, continuous communication is used. WBFP balances computation and communication for Neural Networks optimally, and Hybrid Communication was proposed to also reduce communication overhead.
			\item \textbf{What network topologies exist to partition the computing cluster?} There are 3 main network topologies: trees, centralized storage, and decentralized storage. Trees are used by AllReduce to efficiently obtain a global gradient. Centralized storage is seen in the Parameter Server paradigm, and results in a scalable setting of which the global shared memory is easy to inspect. Decentralized storage is seen in peer-to-peer networks, which provide, in combination with Sufficient Factor Broadcasting, a communication-efficient and scalable environment.
		\end{itemize}
	\end{itemize}
	\item \textbf{What are the currently used implementations?} Many Distributed Machine Learning frameworks are built on top of a framework that manages the large amounts of data (Big Data). GFS is the file system used by Google to handle Big Data, and inspired the creation of an open-source alternative (called HDFS), which is the industry standard for storing big data. On top of GFS / HDFS, several data processing frameworks are built, including MapReduce and Hadoop (which process data using the Bulk-Synchronous Processing (BSP) paradigm), and Apache Spark (which can run more advanced programs than MapReduce tasks, and operate entirely in memory).
	\begin{itemize}
		\item \textbf{Which domain-specific implementations exist?}
		Domain-specific implementations can be categorized by the architecture they use. The two major architectures are synchronous and asynchronous Stochastic Gradient Descent (SGD), which each strike a different balance between raw efficiency and fault-tolerance. Some hybrid architectures also exist, with the purpose of leveraging the benefits of either style, but these have not seen the same level of real-world application as their non-hybrid counterparts.
		\item \textbf{What are the current challenges with these implementations?}
		\begin{itemize}
			\item \textbf{What are the performance challenges?} In the past, companies have had to increase the number of machines quadratically to achieve a linear increase in performance. With state-of-the-art machine learning systems like Tensorflow, the scaling is still below 75\%, although synchronous SGD-based frameworks can achieve almost linear scaling at small cluster sizes.
			\item \textbf{What are the fault-tolerance challenges?} At large cluster sizes, the probability of a single node failure is high. This has lead to High-Performance Computing-inspired patterns, such as checkpointing. In practice, companies have to balance performance with fault tolerance, because no good hybrid approaches have been developed yet.
			\item \textbf{What are the privacy challenges?} Taking political decision into account, some data is not allowed to move past country borders or even move at all. Distributed Ensemble Models, Parameter Server-based systems, and statistical noise offer improved privacy, but perfect privacy is not focused on by current frameworks.
		\end{itemize}
	\end{itemize}
\end{itemize}
