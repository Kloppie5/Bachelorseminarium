\section{Conclusion}
This paper reviews the current state-of-the-art of Distributed Machine Learning. To answer this main research question, all sub-research questions are answered:
\begin{itemize}
	\item \textbf{What are the advantages of Distributed Machine Learning over (centralized) Machine Learning?}
	\begin{itemize}
		\item \textbf{Which alternatives exist to Distributed Machine Learning?} We discussed several ways to learn from huge amounts of data, including using specialized hardware like ASICs or increasing the computation power per computer (for example with the Epiphany architecture)
		\item \textbf{How likely are they to remain sufficient for emerging applications?} While Distributed Machine Learning will provide the highest amount of power in the future because it scales in the best case almost horizontally, alternatives may also be used in the future for simpler tasks that are too heavy to run on an ordinary machine but for which a Distributed Machine Learning cluster may be overkill.
	\end{itemize}
	\item \textbf{What technology is used for Distributed Machine Learning?} We created an overview of the algorithms that a general Distributed Machine Learning could use and how they relate to each other:
	\begin{itemize}
		\item \textbf{What algorithms and with which parameters are used to perform the actual Machine Learning?} We divided ML algorithms into categories based on the type of feedback given to the algorithm, its goal, its type and its method.
		\begin{itemize}
			\item \textbf{What algorithms are used to find the best parameters for the algorithms?} We listed several "workhorse" algorithms that optimize the parameters for ML algorithms, categorized in First-order techniques, second-order techniques, coordinate descent, Markov-Chain Monte Carlo and Variational inference.
			\item \textbf{What algorithms are used to perform the Machine Learning?} For every category, this paper lists several ML methods, with the notable mention of Artificial Neural Networks (ANNs) and Ensemble Learning.
		\end{itemize}
		\item \textbf{How can these algorithms be partitioned and distributed across a wide variety of hardware?} To partition and distribute Machine Learning, there are 5 sub-research questions that need to be answered:
		\begin{itemize}
			\item \textbf{What is the trade-off between computation time VS communication VS accuracy?} Dependent on the characteristics of the dataset and the model being trained, communication can become the bottleneck that limits the level of accuracy that can be achieved by greatly reducing the computation time with more communication.
			\item \textbf{How to schedule and balance the workloads?} To schedule and balance the workloads, we have to take 3 important decisions: which tasks go or don't go in parallel? What is the order in which tasks will be executed? How do we ensure an even load distribution across the machines?
			\item \textbf{How to bridge computation and communication?} There's a trade-off between correct model convergence and faster updates. BSP and SSP focus on correct model convergence but have a lot of communication overhead because of synchronization barriers. ASP is very communication efficient, but is hard to choose the best parameters for. BAP / TAP are also very communication efficient but provide slow or even incorrect model convergence.
			\item \textbf{What strategies can be used to optimize communication overhead?} To prevent bursts of communication, continuous communication is used. WBFP balances computation and communication for Neural Networks optimally and to reduce its communication overhead Hybrid Communication was proposed.
			\item \textbf{What network topologies exist to partition the computing cluster?} There are 3 main network topologies: trees, centralized storage and decentralized storage. Trees are used by AllReduce for to obtain efficiently a global gradient. Centralized Storage is implemented by the Parameter Server paradigm and results in a scalable setting of which the global shared memory is easy to inspect. Decentralized storage is implemented in peer-to-peer networks that provide, in combination with Sufficient Factor Broadcasting, a communication efficient and very scalable environment.
		\end{itemize}
	\end{itemize}
	\item \textbf{What are the currently used implementations?} Most Distributed Machine Learning frameworks are built on top of a framework that manages the big amounts of data (Big Data). GFS is the file system used by Google to handle Big Data and that lead to creation of the open-source alternative called Hadoop, which filesystem called HDFS is the industry standard for storing big data. On top of GFS / HDFS, several data processing frameworks are built, including MapReduce that processes data in the Bulk-Synchronous Processing (BSP) style, and Apache Spark that can run more advanced programs than MapReduce tasks entirely in memory.
	\begin{itemize}
		\item \textbf{Which domain-specific implementations exist?}
		\item \textbf{What are the current challenges with these implementations?}
		\begin{itemize}
			\item \textbf{What are the performance challenges?} In the past, companies had to increase the number of machines quadratically to get a linear increase in performance. Also with state-of-the-art machine learning systems like Tensorflow, the scaling is still below 75\%, although at a smaller scale SGD-based frameworks can achieve almost linear scaling.
			\item \textbf{What are the fault-tolerance challenges?} At large scales the chance of a single node failure is pretty big, which lead to HPC-inspired patterns that use for example checkpointing. In practice companies have to tradeoff performance with fault tolerance, because no good hybrid approaches have been developed yet.
			\item \textbf{What are the privacy challenges?} Taking political decision into account, some data is not allowed to move past country borders or even be moved at all. Distributed Ensemble Models, Parameter server-based system and statistical noise offer improved privacy, but perfect privacy is not offered yet by current frameworks.
		\end{itemize}
	\end{itemize}
\end{itemize}
