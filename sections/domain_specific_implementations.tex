\subsubsection{Domain-specific implementations}
With the rising popularity of Machine Learning in many industries, a variety of domain-specific frameworks have been developed that have distribution models tailored towards ML. In this section, we summarize the characteristics of the most popular implementations.

\paragraph{Distributed ensemble learning}
Many frameworks already exist for the development of Machine Learning models. However, they often have limited support for distributed training, even though they are fast and effective when used on a single machine or high-bandwidth cluster. One way to achieve distribution with these frameworks is through training separate model instances for disjoint subsets of the available data. At prediction time, the outputs of those instances can then be combined through standard ensemble model aggregation\citep{Opitz1999}.

Models that follow this strategy are not dependent on any specific library. They lend themselves well to execution that uses standard distributed systems frameworks. The training process involves executing the individual model trainings on a number of independent machines in parallel. Neither orchestration nor communication are necessary once training has started. Training on $m$ machines with $m$ disjoint subsets of the dataset results in $m$ different models that are serialized somewhere. Each of these can have separate parameters, so-called hyperparameters, and a layer architecture. At prediction time, all trained model instances can then be deserialized and run on new data. This can happen again in a distributed fashion, through data- and/or model-parallelism.

One large drawback is that this method is dependent on proper subdivision of the training data. If large biases are present in the training sets of some of the models, those instances could have a negative overall effect at prediction time. If the data is divided manually, it's important to ensure independence and identical distribution. If, on the other hand, the dataset is inherently divided and distributed, this is not quite as straightforward.

There's a large number of existing frameworks available for this method, as any machine learning framework can be used. Some popular implementations are Tensorflow\citep{Tensorflow2015}, MXNet\citep{MXNet2015} and PyTorch\citep{PyTorch2017}. Further analysis of these frameworks is out of scope.

\paragraph{Parallel synchronous SGD}


\subparagraph{Tensorflow \& Baidu AllReduce \citep{BaiduAllReduce2017}}

uses common high performance compute technology (mainly Message Passing Interface (MPI) and its AllReduce operation) to iteratively run SGD model training on separate mini-batches of the training data. AllReduce is used to, after each operation, apply each of the workers’ gradients onto the last common model state, and then propagate the result of that operation back to each worker. This is an inherently synchronous process, blocking on the result of each workers’ training iteration before continuing to the next.

Baidu includes a further optimization from \citet{Patarasuk2009} in this process called a Ring AllReduce to reduce the required amount of communication for executing the AllReduce operation. Whereas the naive approach to AllReduce (sending all workers’ state to a central node, which executes the reduction and sends back the result to each worker) would scale in bandwidth linearly with the number of nodes, the Ring AllReduce approach would avoid this by connecting each node only with one neighbor for receiving data, and one neighbor for sending data to. Each node would then execute a reduction on a subset of the reduced data array and forward the result of that to the next node, as long as necessary until each subset of the array has a fully reduced result available on one node. The results are then cycled through the ring step-by-step until each node has each subset of the result available to it. Each communication step can be parallelized (utilizing all available bandwidth and putting the bottleneck of communication time at the latency of the slowest link between neighbor nodes) without risk of contention and related efficiency losses.

Baidu claims linear speedup when applying this technique to train deep learning networks. However, it has only been demonstrated on relatively small clusters (5 nodes each, though each node has multiple GPUs that communicate with each other through the same system). Also, although not explicitly mentioned, it is likely that their experimental setup makes use of high bandwidth links between each node, as opposed to the commodity machine approach that many other frameworks offer. The approach lacks fault tolerance by default, as no node in the ring can be missed. This could be counteracted using redundancy (at cost of efficiency) but if this is not done the scalability of this method is limited by the probability that every node is available. This probability can be low when using large numbers of commodity machines and networking to facilitate large datasets.\\
The system has been integrated into Tensorflow as an alternative to its Parameter Server-based approach (described below).

\subparagraph{Horovod\citep{Horovod2018}}
takes a very similar approach to that of Baidu and its custom Tensorflow MPI module: it adds a layer of AllReduce-based MPI training to Tensorflow. One difference is that Horovod uses the NVIDIA Collective Communications Library (NCCL) for increased efficiency when training on (Nvidia) GPUs. This also enables use of multiple GPUs on a single node. Data-parallelizing an existing Tensorflow model is relatively simple (because only a few lines of code need to be added) after which the model can be optimized through synchronous SGD in the same AllReduce-based manner as proposed by Baidu. When benchmarked on Inception v3\citep{Szegedy2015} and ResNet-101\citep{He2015} using 128 GPUs, the efficiency of the distribution is about 88\% compared to about 50\% in Tensorflow's Parameter Server approach.\\
However, Horovod lacks fault tolerance (just like in Baidu's approach) and results therefore in the same scalability issues.

\subparagraph{Caffe2}
(primarily maintained by Facebook) distributes ML (data-parallel) through, once again, AllReduce. It does this by using NCCL between GPUs on a single host and custom code between hosts that uses Facebook's Gloo library, which provides an abstraction for multiple transports (e.g. TCP/IP or InfiniBand). Facebook uses Ring AllReduce (which offers better bandwidth \& parallelism guarantees) but also recursive halving and doubling (a divide-and-conquer approach that offers better latency guarantees). According to their paper, performance is improved in latency-limited situations such as for small buffer sizes and large server counts. The end result of Facebook’s approach is that they can train ResNet-50\citep{He2015} in the span of 1 hour\citep{Goyal2017}, achieving linear scaling with the number of GPUs, at 90\% efficiency and measured up to 352 GPUs. However, once again no fault-tolerance is present, which raises some questions on the topic of scalability.

\subparagraph{CNTK}
offers multiple modes of data-parallel distribution that use the Ring AllReduce tactic previously described with the same tradeoff (namely achieving linear scalability at the expense of fault-tolerance). The library offers 2 innovations:
\begin{itemize}
	\item \textbf{1-bit stochastic gradient descent} (\citet{Seide2014}) is an implementation of SGD that quantizes training gradients to a single bit per value. This reduces the number of bits that need to be communicated when doing distributed training by a large constant factor.
	\item \textbf{Block-momentum SGD} (\citet{Chen2016}) divides the training set into m blocks and n splits. Each of the n machines trains a split on each block. Then the gradients calculated for all splits within a block are averaged to obtain the weights for the block. Finally, the block updates are merged into the global model while applying block-level momentum and learning rate.
\end{itemize}

When benchmarked on a Microsoft speech LSTM, average speedups of 85\%+ are achieved for small numbers of GPUs (up to 16) but scalability drops significantly (below 70\%) when scaling past that \footnote{Direct comparison of this number to the other synchronous frameworks' results is somewhat unfair, as the dependency structure of an LSTM is significantly different than that of an ordinary DNN due to the introduction of temporal state.}.


\paragraph{Parallel asynchronous SGD \& Parameter Servers}

\subparagraph{DistBelief \citep{DistBelief2012}}
is one of the early practical implementations of large-scale distributed ML and was developed by Google. They encountered the limitations of GPU training, and built DistBelief to counteract them. DistBelief supports data- and model-parallel training on tens of thousands of CPU cores (though GPU support was later introduced as well\citep{Tensorflow2016}). At the time of the paper's writing, it was used to successfully train models 30x larger than reported in preceding literature.

At the time, Google also evaluated solutions based on MapReduce\citep{MapReduce} and GraphLab\citep{GraphLab}. MapReduce ”was ill-suited for the iterative computations inherent in deep network training” and GraphLab “would not exploit computing efficiencies available in the structured graphs typically found in deep networks”. This led the company to develop a domain-specific alternative instead.

To achieve efficient model-parallelism, DistBelief exploits the graphical nature of neural networks. Every machine executes the training of a part of the model which can span subsets of multiple layers. Communication is only required at those points where a node’s output is used as the input of a node trained by another machine. To define a DistBelief model, “[t]he user defines the chilimbiomputation that takes place at each node in each layer of the model, and the messages that should be passed during the upward and downward phases of computation.” Partitioning the model across a cluster is transparent and requires no structural modifications. However, the efficiency of a given partitioning depends on the model’s connectivity structure and computational needs and requires careful design. For example, locally connected models lend themselves better for model parallelism because of limited cross-partition communication. On the other hand, fully connected models have more substantial cross-partition dependencies and are therefore harder to efficiently distribute through DistBelief.

To further parallelize model training, data parallelism is applied on top of the model parallelism. A centralized sharded Parameter Server is used to allow each of a set of model replicas (which may  be model-parallel internally) to share parameters. DistBelief supports 2 methods of data parallelism, both of which are resilient to processing speed variance between model replicas as well as replica failure:
\begin{itemize}
	\item \textbf{Downpour SGD} is an asynchronous alternative to the inherently sequential SGD. Each replica of the model fetches the latest model parameters from the Parameter Server every $n_{fetch}$ steps, updates these parameters in accordance with the model, and pushes the tracked parameter gradients to the Parameter Server every $n_{push}$ steps. Simple implementations would use $n_{fetch} = n_{push} = 1$ steps, causing it to fetch the parameters before every training iteration and push the iteration’s gradient once it is available. Communication overhead can be reduced by increasing either or both parameters. Each of the fetches and pushes could also be executed on a separate thread, which would require only weak synchronization between each other and the training thread.
	
	The downpour SGD algorithm primarily used in DistBelief is more resilient to machine failures than SGD, as it allows the training to continue even if some model replicas are off-line. However, the optimization process itself becomes less predictable due to parameters that are out of sync on the model replicas or between shards of the parameter server. No theoretical guarantees or citations are offered to support the robustness of this approach, yet the authors “found relaxing consistency requirements to be remarkably effective.” Tactics that contribute to robustness are the application of adaptive learning rates through AdaGrad\citep{Duchi2011} and "warmstarting" the model through training a single model replica for a while before scaling up to the full number of machines. The authors make note of the absence stability issues after applying these.
	\item \textbf{Distributed L-BGFS} makes use of an external coordinator process that divides training work between model replicas, as well as some operations on the parameters between the parameter server shards.
	
	Each of the shards of the parameter server hold a fraction of the total parameter space of the model being trained. The model replicas pull the parameters from all shards; each parallelized part of the model only retrieves those parameters that it needs.
	
	Performance improvements are high but very expensive in terms of computational complexity: although the best speedup (downpour SGD with AdaGrad) achieved an 80\% decrease in training time on ImageNet, this was achieved by using more than 500 machines and more than 1000 CPU cores. However, DistBelief did not support distributed GPU training at the time of \citet{DistBelief2012} which could reduce the required resources significantly and is used in fact in almost all other implementations mentioned in this section.
\end{itemize}

\subparagraph{Tensorflow \citep{Tensorflow2015}\citep{Tensorflow2016}}
is the evolution of DistBelief, in the sense that it was developed to replace DistBelief within Google, and borrows both concepts and experience from it. It also applies subsequent optimizations to the parameter server model like \citet{Chilimbi14} and \citet{Li2014Comms}\citep{Li2014Scaling}. By "both simplifying and generalizing [DistBelief]"\citep{Tensorflow2016}, the Tensorflow developers intend to cater to a wider variety of ideas.

TensorFlow represents both model algorithms and state as a dataflow graph, execution of which can be distributed. This facilitates different parallelization schemes that can take for example state locality into account. The level of abstraction of the dataflow graph is that of mathematical operations on tensors (i.e. $n$-dimensional matrices) as opposed to DistBelief, which abstracts at the layer level. Consequently, defining a new type of neural network layer in Tensorflow requires no custom code - it can be represented as a subgraph of a larger model, composed of simpler math operations.

A Tensorflow model is first defined as a symbolic dataflow graph. Once this graph has been constructed, it is optimized and then executed on the available hardware. One advantage of this model is that it allows Tensorflow to tailor its execution strategy towards the types of devices available to it. When working with for example GPUs or TPUs (Tensor Processing Units\citep{TPU2017}), Tensorflow can take into account the asynchronicity and intolerance to branching that is inherent to these devices without requiring any changes to the model itself.

\citet{Shaohuai2017} shows Tensorflow achieving sub-40\% efficiency on 4-node, InfiniBand-connected cluster training of ResNet-50\citet{He2015}, and about 75\% efficiency on GoogleNet\citep{Szegedy2014}. 


\subparagraph{MXNet \citep{MXNet2015}}
uses a strategy very similar to that of Tensorflow: models are represented as dataflow graphs which are executed on hardware that is abstracted away and coordinated by using a parameter server. However, MXNet also supports the imperative definition of dataflow graphs as operations on n-dimensional arrays which simplifies the implementation of certain kinds of networks.

MXNet's parameter server is used in the form of an abstraction: the KVStore. The KVStore supports pushing key-value pairs from a device to the store, and pulling the current value of a key from the store. There is support for user-defined update logic that is executed when a new value is pushed, and for multiple consistency models enforced by the KVStore (currently limited to sequential and eventually consistent execution). The KVStore is a two-tier system: updates by multiple threads and GPUs are merged on the local machine before they're pushed to the full cluster.

The KVStore abstraction theoretically enables the implementation of (stale-)synchronicity, although only an asynchronous implementation is present at the time of writing. 

On a small cluster (10 machines), MXNet achieves super-linear speedup compared to a single machine when training GoogleNet\citep{Szegedy2014} with more than 10 passes over the data. \citet{Shaohuai2017}, however, show its efficiency to dip to 65\% when trained on more than one machine using GoogleNet\citep{Szegedy2014}, and 50\% using ResNet-50\citet{He2015}. Performance here was evaluated on a single epoch (after warm-up) on nodes linked using Infiniband, but it does suggest that \citet{MXNet2015}'s achieved performance needs some tuning. The paper lacks information on the exact process required.

\subparagraph{CNTK}
again uses a strategy similar to MXNet and Tensorflow. It makes use of dataflow graphs for model definition. Instead of the integrated parameter server approach of the other libraries, CNTK uses a separated parameter server called Multiverso, which can also be used by other frameworks.

Performance (both on a single node and on a cluster) seems to be the main factor at which CNTK attempts to differentiate itself. \citet{Shaohuai2017} shows CNTK achieving very competitive results of about 75\% efficiency on ResNet-50\citep{He2015} in a 4-node, InfiniBand-connected cluster configuration, outperforming both Tensorflow and MXNet by a significant margin. 

\paragraph{Parallel stale-synchronous SGD}

\subparagraph{Petuum \citep{Xing2013}}
aims to provide a generic platform for any type of machine learning (as long as it is iteratively convergent) on big data and big models (hundreds of billions of parameters). It supports data- and model-parallelism. The Petuum approach exploits ML’s error tolerance, dynamic structural dependencies, and non-uniform convergence in order to achieve good scalability on large datasets and models. This is in contrast to for example Spark (that focuses on fault tolerance \& recovery) and GraphLab (that focuses on consistency) which can still be important to ML but perform less efficient than Petuum's approach. The platform uses stale synchronicity to exploit error tolerance (since a minor amount of staleness will have minor effects on convergence), dynamic scheduling policies to exploit dynamic structural dependencies (which helps minimize parallelization error and synchronization cost) and unconverged parameter prioritization to take advantage of non-uniform convergence (by reducing computational cost on parameters that are already near optimal). 

Petuum uses the parameter server model (as proposed by \citet{DistBelief2012}) to keep track of the parameters of the model being trained. The parameter server is also responsible for maintaining the staleness guarantees. In addition, it exposes a scheduler that lets the model developer control the ordering of parallelized model updates.

The programming model that Petuum used is different to that of DistBelief in a major way: data- and model-parallelism operate at the same tier. Whereas in DistBelief model-parallelism is managed inside model shards and each model replica operates on the parameter server as a single entity, Petuum has both model- and data-slices interact with the parameter server directly and requires the user to define some central update logic if model-parallelism is involved. This allows slightly more control by the programmer.

When developing a model using Petuum, developers have to implement at least a method named "push", which is responsible for each of the parallelized model training operations. Its implementation should pull the model state from the parameter server, run a training iteration, and push a gradient to the parameter server. Petuum by default manages the scheduling aspect and the parameter merging logic automatically, so that data-parallel models don’t require any additional operations. However, if you want model-parallelism, you need to also implement the schedule method (which tells each of the parallel workers which parameters to train) and the pull method (which defines the aggregation logic for each of the parallelly generated parameter gradients).

Petuum provides an abstraction layer that also allows it to run on systems using YARN and HDFS which simplifies compatibility with pre-existing clusters. 


\paragraph{Parallel hybrid-synchronous SGD}
Both synchronous and asynchronous approaches have some significant drawbacks, as is explored by \citet{ChenJianmin2016}. A few frameworks attempt to find a middle ground instead that combines some of the best properties of each model of parallelism and diminishes some of the drawbacks.

\subparagraph{MXNet-MPI \citep{Mamidala2018}}
takes an approach to distributed ML (using a modified version of MXNet as a proof of concept) that combines some of the best aspects of both asynchronous (Parameter Server) and synchronous (MPI) implementations. The idea here is to use the same approach as described in the MXNet section, but instead of having single workers communicate with the parameter server, cluster those workers together into groups that internally apply synchronous SGD over MPI/AllReduce. This has the benefit of easy linear scalability of the sync MPI approach and fault tolerance of the asynchronous parameter server approach.


