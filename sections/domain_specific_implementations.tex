\subsubsection{Domain-specific implementations}
As a result of rising popularity of Machine Learning in many industries, several domain-specific frameworks have been developed that have distribution models tailored towards ML. In this section, the characteristics of the most popular implementations are summarized.

\paragraph{Distributed ensemble learning}
Many frameworks already exist for the development of Machine Learning models. However, they often have limited support for distributed training, even though they are fast and effective on a single machine. One way to achieve distribution with these frameworks is through training separate models for subsets of the available data. At prediction time, the outputs of those instances can then be combined through standard ensemble model aggregation\citep{Opitz1999}.

Models that follow this strategy are not dependent on any specific library. They can be orchestrated using existing distribution frameworks (such as MapReduce\citet{MapReduce}). The training process involves training individual models on independent machines in parallel. Neither orchestration nor communication are necessary once training has started. Training on $m$ machines with $m$ subsets of the data results in $m$ different models. Each of these can use separate parameters or even algorithms. At prediction time, all trained models can then be run on new data, after which the output of each one is aggregated. This can once again be distributed if needed.

One large drawback is that this method is dependent on proper subdivision of the training data. If large biases are present in the training sets of some of the models, those instances could cause biased output of the ensemble. If the data is divided manually, it's important to ensure independence and identical distribution (i.i.d.). If, on the other hand, the dataset is inherently distributed, this is not quite as straightforward.

There's a large number of existing frameworks available for this method, as any Machine Learning framework can be used. Some popular implementations are Tensorflow\citep{Tensorflow2015}, MXNet\citep{MXNet2015} and PyTorch\citep{PyTorch2017}. Further analysis of these frameworks is out of scope.

\paragraph{Parallel synchronous SGD}

Implementing synchronized parallelism is often the most straightforward. Existing distribution libraries (such as Message Passing Interface (MPI)) can often be reused, and it is easy to reason about statistical properties. 

\subparagraph{Tensorflow \& Baidu AllReduce \citep{BaiduAllReduce2017}}

uses common high performance compute technology (mainly MPI and its AllReduce operation) to iteratively run SGD model training on separate mini-batches of the training data. AllReduce is used to, after each operation, apply each of the workers’ gradients onto the last common model state and then propagate the result of that operation back to each worker. This is an inherently synchronous process, blocking on the result of each workers’ training iteration before continuing to the next.

Baidu includes a further optimization from \citet{Patarasuk2009} in this process, called a Ring AllReduce, to reduce the required amount of communication. By structuring the cluster of machines as a ring (with each node having only 2 neighbours) and cascading the reduction operation, it is possible to utilize all bandwidth optimally. The bottleneck, then, is the highest latency between neighbouring nodes.

Baidu claims linear speedup when applying this technique to train deep learning networks. However, it has only been demonstrated on relatively small clusters (5 nodes each, though each node has multiple GPUs that communicate with each other through the same system). The approach lacks fault tolerance by default, as no node in the ring can be missed. This could be counteracted using redundancy (at cost of efficiency). If this is not done, however, the scalability of the method is bounded by the probability of all nodes being available. This probability can be low when using large numbers of commodity machines and networking, which is needed to facilitate Big Data.\\
Baidu's system has been integrated into Tensorflow as an alternative to the built-in Parameter Server-based approach (described below).

\subparagraph{Horovod\citep{Horovod2018}}
takes a very similar approach to that of Baidu and its custom Tensorflow MPI module: it adds a layer of AllReduce-based MPI training to Tensorflow. One difference is that Horovod uses the NVIDIA Collective Communications Library (NCCL) for increased efficiency when training on (Nvidia) GPUs. This also enables use of multiple GPUs on a single node. Data-parallelizing an existing Tensorflow model is relatively simple (because only a few lines of code need to be added) after which the model can be optimized through synchronous SGD in the same AllReduce-based manner as proposed by Baidu. When benchmarked on Inception v3\citep{Szegedy2015} and ResNet-101\citep{He2015} using 128 GPUs, the efficiency of the distribution is about 88\% compared to about 50\% in Tensorflow's Parameter Server approach.\\
However, Horovod lacks fault tolerance (just like in Baidu's approach) and therefore suffers from the same scalability issues.

\subparagraph{Caffe2}
(primarily maintained by Facebook) distributes ML through, once again, AllReduce. It does this by using NCCL between GPUs on a single host, and custom code between hosts that uses Facebook's Gloo library to abstract away different interconnects. Facebook uses Ring AllReduce (which offers better bandwidth \& parallelism guarantees) but also recursive halving and doubling (a divide-and-conquer approach that offers better latency guarantees). According to their paper, this improves performance in latency-limited situations, such as for small buffer sizes and large server counts. \citet{He2015} managed to train ResNet-50 in the span of 1 hour\citep{Goyal2017} using this approach, achieving linear scaling with the number of GPUs. They achieved 90\% efficiency, measured up to 352 GPUs. However, once again no fault-tolerance is present. This raises some questions on the topic of scalability.

\subparagraph{CNTK}
offers multiple modes of data-parallel distribution. Many of them use the Ring AllReduce tactic as previously described, making the same trade-off of linear scalability over fault-tolerance. The library offers 2 innovations:
\begin{itemize}
	\item \textbf{1-bit stochastic gradient descent} (\citet{Seide2014}) is an implementation of SGD that quantizes training gradients to a single bit per value. This reduces the number of bits that need to be communicated when doing distributed training by a large constant factor.
	\item \textbf{Block-momentum SGD} (\citet{Chen2016}) divides the training set into m blocks and n splits. Each of the n machines trains a split on each block. Then the gradients calculated for all splits within a block are averaged to obtain the weights for the block. Finally, the block updates are merged into the global model while applying block-level momentum and learning rate.
\end{itemize}

When benchmarked on a Microsoft speech LSTM, average speedups of 85\%+ are achieved for small numbers of GPUs (up to 16), but scalability drops significantly (below 70\%) when scaling past that \footnote{Direct comparison of this number to the other synchronous frameworks' results is somewhat unfair, as the dependency structure of an LSTM is significantly different than that of an ordinary DNN due to the introduction of temporal state.}.


\paragraph{Parallel asynchronous SGD \& Parameter Servers}

Asynchronous approaches tend be more complex to implement, because more components are needed. It can also be difficult to reason about average-case behavior. However, asynchronicity alleviates many problems that occur in clusters with high failure rates or inconsistent performance.

\subparagraph{DistBelief \citep{DistBelief2012}}
is one of the early practical implementations of large-scale distributed ML, and was developed by Google. They encountered the limitations of GPU training and built DistBelief to counteract them. DistBelief supports data- and model-parallel training on tens of thousands of CPU cores (though GPU support was later introduced as well\citep{Tensorflow2016}). At the time of the paper's writing, it was used to successfully train models 30x larger than reported in preceding literature.

At the time, Google also evaluated solutions based on MapReduce\citep{MapReduce} and GraphLab\citep{GraphLab}. MapReduce ”was ill-suited for the iterative computations inherent in deep network training” and GraphLab “would not exploit computing efficiencies available in the structured graphs typically found in deep networks”. This led the company to develop a domain-specific alternative instead.

To achieve efficient model-parallelism, DistBelief exploits the graphical nature of neural networks. Every machine executes the training of a part of the model, which can span subsets of multiple layers. Communication is only required at those points where a node’s output is used as the input of a node trained by another machine. To define a DistBelief model, “[t]he user defines the computation that takes place at each node in each layer of the model, and the messages that should be passed during the upward and downward phases of computation.” Partitioning the model across a cluster is transparent and requires no structural modifications. However, the efficiency of a given partitioning depends on the model’s connectivity structure and computational needs, and requires careful design. For example, locally connected models lend themselves better for model-parallelism because of limited cross-partition communication. In contrast, fully connected models have more substantial cross-partition dependencies and are therefore harder to efficiently distribute through DistBelief.

To further parallelize model training, data parallelism is applied on top of the model parallelism. A centralized sharded Parameter Server is used to allow each of a set of model replicas (which may  be model-parallel internally) to share parameters. DistBelief supports 2 methods of data parallelism, both of which are resilient to processing speed variance between model replicas as well as replica failure:
\begin{itemize}
	\item \textbf{Downpour SGD} is an asynchronous alternative to the inherently sequential SGD. Each replica of the model fetches the latest model parameters from the Parameter Server every $n_{fetch}$ steps, updates these parameters in accordance with the model, and pushes the tracked parameter gradients to the Parameter Server every $n_{push}$ steps. The parameters can be increased to achieve lower communication overhead. Fetching and pushing can happen as a background process, allowing training to continue.
	
	Downpour SGD is more resilient to machine failures than SGD, as it allows the training to continue even if some model replicas are off-line. However, the optimization process itself becomes less predictable due to parameters that are out of sync. The authors “found relaxing consistency requirements to be remarkably effective”, but offer no theoretical support for this. Tactics that contribute to robustness are the application of adaptive learning rates through AdaGrad\citep{Duchi2011} and "warmstarting" the model through training a single model replica for a while before scaling up to the full number of machines. The authors make note of the absence of stability issues after applying these.
	\item \textbf{Distributed L-BGFS} makes use of an external coordinator process that divides training work between model replicas, as well as some operations on the parameters between the parameter server shards. Training happens through L-BGFS, as is clear from the name.
\end{itemize}
	
Each of the shards of the Parameter Server hold a fraction of the parameter space of a model. The model replicas pull the parameters from all shards; each parallelized part of the model only retrieves those parameters that it needs.

Performance improvements are high but very expensive in terms of computational complexity: although the best speedup (downpour SGD with AdaGrad) achieved an 80\% decrease in training time on ImageNet, this was achieved by using more than 500 machines and more than 1000 CPU cores. However, DistBelief did not support distributed GPU training at the time of \citet{DistBelief2012} which could reduce the required resources significantly and is used in fact in almost all other implementations mentioned in this section.

\subparagraph{Tensorflow \citep{Tensorflow2015}\citep{Tensorflow2016}}
is the evolution of DistBelief, developed to replace DistBelief within Google. It borrows both concepts and experience from it. It also applies subsequent optimizations to the parameter server model, such as \citet{Chilimbi14} and \citet{Li2014Comms}\citep{Li2014Scaling}. By "both simplifying and generalizing [DistBelief]"\citep{Tensorflow2016}, the Tensorflow developers intend to cater to a wider variety of ideas.

TensorFlow represents both model algorithms and state as a dataflow graph, execution of which can be distributed. This facilitates different parallelization schemes that can take e.g. state locality into account. The level of abstraction of the dataflow graph is mathematical operations on tensors (i.e. $n$-dimensional matrices). This in contrast to DistBelief, which abstracts at the layer level. Consequently, defining a new type of neural network layer in Tensorflow requires no custom code - it can be represented as a subgraph of a larger model, composed of fundamental math operations.

A Tensorflow model is first defined as a symbolic dataflow graph. Once this graph has been constructed, it is optimized and then executed on the available hardware. This execution model allows Tensorflow to tailor its operations towards the types of devices available to it. When working with e.g. GPUs or TPUs (Tensor Processing Units\citep{TPU2017}), Tensorflow can take into account the asynchronicity and intolerance to branching that is inherent to these devices, without requiring any changes to the model itself.

\citet{Shaohuai2017} shows Tensorflow achieving sub-40\% efficiency on 4-node, InfiniBand-connected cluster training of ResNet-50\citet{He2015}, and about 75\% efficiency on GoogleNet\citep{Szegedy2014}. 


\subparagraph{MXNet \citep{MXNet2015}}
uses a strategy very similar to that of Tensorflow: models are represented as dataflow graphs, which are executed on hardware that is abstracted away and coordinated by using a parameter server. However, MXNet also supports the imperative definition of dataflow graphs as operations on n-dimensional arrays, which simplifies the implementation of certain kinds of networks.

MXNet's Parameter Server comes the form of an abstraction: the KVStore. The KVStore supports pushing key-value pairs from a device to the store, as well as pulling the current value of a key from the store. There is support for user-defined update logic that is executed when a new value is pushed. The KVStore can also enforce different consistency models (currently limited to sequential and eventually consistent execution). It is a two-tier system: updates by multiple threads and GPUs are merged on the local machine before they're pushed to the full cluster.

The KVStore abstraction theoretically enables the implementation of (stale-)synchronicity, although only an asynchronous implementation is present at the time of writing. 

On a small cluster (10 machines), MXNet achieves super-linear speedup compared to a single machine when training GoogleNet\citep{Szegedy2014} with more than 10 passes over the data. \citet{Shaohuai2017}, however, show its efficiency to dip to 65\% when trained on more than one machine using GoogleNet\citep{Szegedy2014}, and 50\% using ResNet-50\citet{He2015}. Performance here was evaluated on a single epoch (after warm-up) on nodes linked using Infiniband, but it does suggest that \citet{MXNet2015}'s achieved performance required some tuning. The paper lacks information on the exact process.

\subparagraph{CNTK}
again uses a strategy similar to MXNet and Tensorflow. It makes use of dataflow graphs for model definition. Instead of the integrated Parameter Server approach of the other libraries, CNTK uses a separated Parameter Server called Multiverso, which can also be used by other frameworks.

Performance (both on a single node and on a cluster) seems to be the main factor at which CNTK attempts to differentiate itself. \citet{Shaohuai2017} shows CNTK achieving very competitive results of about 75\% efficiency on ResNet-50\citep{He2015} in a 4-node, InfiniBand-connected cluster configuration, outperforming both Tensorflow and MXNet by a significant margin. 

\paragraph{Parallel stale-synchronous SGD}

\subparagraph{Petuum \citep{Xing2013}}
aims to provide a generic platform for any type of machine learning (as long as it is iteratively convergent) on big data and big models (hundreds of billions of parameters). It supports data- and model-parallelism. The Petuum approach exploits ML’s error tolerance, dynamic structural dependencies, and non-uniform convergence in order to achieve good scalability on large datasets and models. This is in contrast to e.g. Spark (which focuses on fault tolerance \& recovery) and GraphLab (which focuses on consistency). These properties can still be important to ML, but are less efficient than Petuum's approach. The platform uses stale synchronicity to exploit error tolerance (since a minor amount of staleness will have minor effects on convergence), dynamic scheduling policies to exploit dynamic structural dependencies (which helps minimize parallelization error and synchronization cost) and unconverged parameter prioritization to take advantage of non-uniform convergence (by reducing computational cost on parameters that are already near optimal). 

Petuum uses the Parameter Server model to keep track of the parameters of the model being trained. The Parameter Server is also responsible for maintaining the staleness guarantees. In addition, it exposes a scheduler that lets the model developer control the ordering of parallelized model updates.

The programming model that Petuum used is different to that of DistBelief in a major way: data- and model-parallelism operate at the same tier. Whereas in DistBelief model-parallelism is managed inside model shards and each model replica operates on the parameter server as a single entity, Petuum has both model- and data-slices interact with the parameter server directly, and requires the user to define some central update logic if model-parallelism is involved. This facilitates slightly more control by the programmer.

When developing a model using Petuum, developers have to implement a method named "push", which is responsible for each of the parallelized model training operations. Its implementation should pull the model state from the parameter server, run a training iteration, and push a gradient to the parameter server. Petuum by default manages the scheduling aspect and the parameter merging logic automatically, so that data-parallel models don’t require any additional operations. However, if model-parallelism is desired, the schedule method (which tells each of the parallel workers what parameters they need to train) and the pull method (which defines the aggregation logic for each of the generated parameter gradients) need to be implemented as well.

Petuum provides an abstraction layer that also allows it to run on systems using YARN and HDFS, which simplifies compatibility with pre-existing clusters. 


\paragraph{Parallel hybrid-synchronous SGD}
Both synchronous and asynchronous approaches have some significant drawbacks, as is explored by \citet{ChenJianmin2016}. A few frameworks attempt to find a middle ground instead that combines some of the best properties of each model of parallelism and diminishes some of the drawbacks.

\subparagraph{MXNet-MPI \citep{Mamidala2018}}
takes an approach to distributed ML (using a modified version of MXNet as a proof of concept) that combines some of the best aspects of both asynchronous (Parameter Server) and synchronous (MPI) implementations. The idea here is to use the same architecture as described in the MXNet section. Instead of having single workers communicate with the parameter server, however, those workers are clustered together into groups that internally apply synchronous SGD over MPI/AllReduce. This has the benefit of easy linear scalability of the synchronous MPI approach and fault tolerance of the asynchronous Parameter Server approach.


