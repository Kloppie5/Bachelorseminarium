\section{Abstract}
\textbf{Background:} The demand for Artificial Intelligence has grown significantly over the last decade and has largely been powered by Machine Learning. To increase the quality of the predictions of Machine Learning algorithms, more and more training data is used (which is available in abundance). However, the amount of training data is increasing rapidly compared to the computational power of machines. This justifies the search for solutions that enable workload distribution over multiple machines. The associated field is called Distributed Machine Learning, and tries to subdivide the workload of a Machine Learning process as effectively and efficiently as possible over (large) clusters of machines.
\textbf{Objective:} We aim to provide an overview of the current state-of-the-art of Machine Learning by answering several sub-research question:
\begin{itemize}
	\item What are the advantages of Distributed Machine Learning over (centralized) Machine Learning?
	\item What technology is used for Distributed Machine Learning?
	\item What are the currently used implementations?
\end{itemize}
\textbf{Methods:} We analyzed based on 4 reference papers\cite{Bal12}\cite{Die12}\cite{Xing16}\cite{Zhang16}, formulated the (sub-)research questions and then answered these question by looking for other relevant papers.
\textbf{Results:} We argue that Distributed Machine Learning is needed to keep up with the increasing amount of training data. Additionally, we give an overview of the most popular methods for Distributed Machine Learning by proposing a general framework that uses regular Machine Learning algorithms / algorithms to optimize the parameters to perform the actual Machine Learning, and uses several algorithms to partition and distribute these algorithms across a wide variety of hardware. Finally, we give an overview of the currently used implementations which include MapReduce-based systems like MLlib and Mahout, graph-based system like GraphLab and Powergraph and Parameter-Server-based systems like Petuum and Tensorflow.