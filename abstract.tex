\section{Abstract}
\textbf{Background:} The demand for Artificial Intelligence has grown a lot over the last decade and has been powered by using Machine Learning. To increase the quality of the predictions of the Machine Learning algorithms, more and more training data is used (which is available in abundance). However, the amount of training data is increasing a lot faster than the computational power of machines, which justifies the need for looking at ways to distribute the workload over multiple machines. This field is called Distributed Machine Learning which tries to spread out the workload of a Machine Learning algorithm as effectively and efficiently as possible over (large) clusters of machines.
\textbf{Objective:} We aim to provide an overview of the current state-of-the-art of Machine Learning by answering several sub-research question:
\begin{itemize}
	\item What are the advantages of Distributed Machine Learning over (centralized) Machine Learning?
	\item What technology is used for Distributed Machine Learning?
	\item What are the currently used implementations?
\end{itemize}
\textbf{Results:} Distributed Machine Learning is needed to keep up with the increasing amount of training data. In this paper an overview is given of the most popular methods for Distributed Machine Learning by proposing a general framework that uses regular Machine Learning algorithms / algorithms to optimize the parameters to perform the actual Machine Learning, and uses several algorithms to partition and distribute these algorithms across a wide variety of hardware. Finally, the paper shows the currently used implementations of Distributed Machine Learning frameworks which include MapReduce-based systems like MLlib and Mahout, graph-based system like GraphLab and Powergraph and Parameter-Server-based systems like Petuum and Tensorflow.