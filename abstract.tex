
\begin{abstract}
\textbf{Background:} The demand for Artificial Intelligence has grown significantly over the last decade, and has largely been powered by Machine Learning. To increase the quality of the predictions of Machine Learning algorithms, more and more training data is used (which is available in abundance). However, the amount of training data is increasing rapidly compared to the computational power of machines. This justifies the search for solutions that enable workload distribution over multiple machines. The associated field is called Distributed Machine Learning, and tries to subdivide the workload of a Machine Learning process as effectively and efficiently as possible over (large) clusters of machines.
\textbf{Objective:} We aim to provide an overview of the current state-of-the-art of Machine Learning by answering several sub-research question:
\begin{itemize}
	\item What are the advantages of Distributed Machine Learning over (centralized) Machine Learning?
	\item What technology is used for Distributed Machine Learning?
	\item What are the currently used implementations?
\end{itemize}
\textbf{Results:} Distributed Machine Learning is needed to keep up with the increasing amount of training data. In this paper, an overview is given of the most popular methods for Distributed Machine Learning by proposing a general framework that uses regular Machine Learning algorithms, parameter optimization methods to perform the actual Machine Learning, and several algorithms to partition and distribute both of those across a wide variety of hardware. Finally, the paper analyzes several currently used implementations of Distributed Machine Learning frameworks, which include MapReduce-based systems like MLlib and Mahout, graph-based system like GraphLab and Powergraph, and Parameter-Server-based systems like Petuum and Tensorflow.

\end{abstract}