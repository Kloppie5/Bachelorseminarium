
\subsubsection{Alternatives to Distributed machine learning}
The most popular solution for increasing computation power for Machine
learning is currently distributing the workload over a large amount of machines. However, there are other, more traditional ways to increase the available computation power. These ways include the use of dedicated Graphics Processing Units (GPUs), Application Specific Integrated Circuits (ASICs) or special multi-core computer architectures.

\paragraph{GPUs in Machine Learning}
The use of GPUs in Machine Learning is already a very common method. This is due to the nature of the data which makes up Machine Learning problems. A GPU is extremely fast in making many small calculations in parallel. Fortunately, the data that is used by Machine Learning algorithms such as Neural Networks is highly parallelizable, which means that GPUs have had a lot of success with speeding up these calculations. For example, Meuth \ref{Meut2007} reported a speed-up up to 200x over conventional CPUs.

\paragraph{Application Specific Integrated Circuits}
The idea of using Application Specific Integrated Circuits (ASICs) in highly specialized tasks is not new to Machine Learning. In recent times, the demand for such chips has risen massively\cite{Metz18}.
When applied to e.g. Bitcoin mining, ASICs have a significant competitive advantage over GPUs and CPUs, which achieve only a fraction of the performance per watt. The expectations of ASICs are high in the context of Machine Learning as well, because many of the computations involved come in the form of matrix multiplications. This is an area in which some ASICs excel.

Google applied this concept in their own Tensor Processing Unit (TPU)\cite{Sato17}, which, as the name suggests, is an ASIC that specializes in calculations that use tensors ($n$-dimensional arrays). The TPU works well when combined with their Tensorflow\cite{Tensorflow2015}\cite{Tensorflow2016} framework, which is one of the most popular building blocks for Machine Learning models.

The most important component of the TPU is its Matrix Multiply unit, which is what differentiates the chip from a CPU/GPU. The TPU can be connected to a server over a PCI-e bus. Many TPUs can be used in a data center, and if the work load is high even for a TPU, multiple units can collaborate in a distributed setting.

The performance improvement of the TPU over regular CPU/GPU setups is not only because of its increased processing power, but also its power efficiency, which is important for large companies that want to minimize energy costs. When running benchmarks, \citet{Joup17} found that the performance per watt of a TPU can approach 200x that of a traditional system.

Further benchmarking by \citet{Sato17} indicated that the total processing power of a TPU or GPU can be up to 70x higher than a CPU in a standard Neural Network, with performance improvements varying from 3.5x - 71x depending on the task at hand.


\paragraph{Computer architectures}
Other than using ASICs in order to increase the amount of work a computer can do,
general-purpose processors can also be built with different architectures that significantly increase the number of usable cores. Such an architecture exists in the form of Epiphany. It is a Multiple Instruction, Multiple Data (MIMD) architecture that uses an array of processors, each of which accessing the
same memory, to speed up execution of floating point operations\cite{Olof16}.

The newest chip of the major manufacturer Adapteva is the Epiphany V, which contains
1024 cores on a single chip\cite{Olof16}. Although Adapteva has not published power consumption specifications of the Epiphany V yet, it has released numbers boasting a power usage of only 2 watt\cite{Adap}.

Using multiple processors to process a single dataset is not a new idea in Computer Science. Message Passing Interface (MPI)\cite{MPI1993} was developed to simplify the distribution of work for this purpose, and is usually applied on a cluster of workers. The same framework, however, can also be adapted to the Epiphany chips, as was demonstrated by \citep{Rich15}. They found that the RISC chips were able to perform matrix operations rapidly until their on-board memory was filled.

\paragraph{Future relevance}
As can be seen in the examples above, there are many different strategies to obtain the processing power needed for large-scale Machine Learning. These techniques focus both on power efficiency and raw computational power. However, it must be noted that distributed systems are often preferred to these non-distributed alternatives above a certain scale, because of higher scalability and lower equipment cost. The techniques described above can still be used in a distributed setting, without suffering major performance degradations. This makes them a good choice for certain classes of future applications, as they can provide the benefits of optimized hardware in a distributed system.
