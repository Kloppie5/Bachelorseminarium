\section{Literature Review}
In this section, the following (sub-)research questions are answered to get an overview of the current state-of-the-art of Distributed Machine Learning:
\begin{itemize}
	\item What are the advantages of Distributed Machine Learning over (centralized) Machine Learning?
	\item What technology is used for Distributed Machine Learning?
	\item What are the currently used implementations?
\end{itemize}

\subsection{The need for Distributed Machine Learning}
Large amounts of data are being gathered for many different use cases, some of which involve Machine Learning. The algorithms for this tend to be resource-intensive, and at a certain point, executing them on a single general-purpose machine is no longer feasible.
Instead, it can be beneficial to distribute the Machine Learning over a cluster of machines. This enables large-scale parallelized training and prediction, which often has the effect of reducing the wall clock time the entire process takes. While limited by Amdahl's law\cite{hill2008amdahl}, the speedup can, dependent on architecture, attain a near-linear correlation with the number of processors (i.e. machines) involved. Since large computing clusters are already commonplace for general Big Data Processing, reusing (parts of) those same systems to accelerate Machine Learning is highly appealing.

\input{alternatives}

\subsection{Underlying technology}
To give you an overview of how Distributed Machine Learning works, we will give you an abstract framework that includes everything a real implementation should include, see Figure \ref{overview_system}.\\
\begin{figure}[H]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{general_system_bac_sem.png}
		\caption{An overview of the algorithms that make up our general system for Distributed Machine Learning}
		\label{overview_system}
	\end{center}
\end{figure}
\newpage
This system exploits three unique properties of Distributed Machine Learning: error tolerance, dynamic structural dependencies, and non-uniform convergence.\cite{Xing16}\\
Our goals include (1) to list regular Machine Learning algorithms that are commonly used in a distributed setting; (2) to find algorithms to determine the best parameters for the aforementioned algorithms; (3) to find a balance between computation time, communication overhead, and accuracy; and (4) to minimize the number of bits sent over the network, so that the system is not bottlenecked by network bandwidth.\\
Designing a generic system that enables distribution of  regular Machine Learning efficiently is challenging, because every algorithm has its own communication patterns \cite{Jia14}\cite{Newman09}\cite{Rich13}\cite{Smola10}\cite{Takac13}\cite{Tsi12}.

\subsubsection{Machine Learning algorithms / parameter optimization}
Machine Learning algorithms learn to make predictions or decisions based on data. Every Machine Learning algorithm has its own pros and cons. The algorithms can be divided into categories based on some of their characteristics:
\begin{itemize}
	\item \textbf{Feedback}, the type of feedback that is given to the algorithm while learning
	\item \textbf{Goal}, the type of output that is generated
	\item \textbf{Type}, the architecture used to generate output
	\item \textbf{Method}, the nature of model evolution that occurs when given feedback
\end{itemize}
We will first look at the ML algorithms themselves, then at ensembling methods to combine these algorithms to improve the accuracy, and finally at other algorithms that optimize the parameters of these ML algorithms.

\paragraph{Machine Learning categories}
\subparagraph{ML category: Feedback}\mbox{}\\
To train an algorithm, it has to get feedback so that it can improve. There are several different types of feedback:
\begin{itemize}
	\item \textbf{Supervised}
		learning uses training data that consists of input objects (usually vectors) and output values. Supervised learning algorithms typically attempt to find a function that maps input data to some output. They then apply this function to find outputs for unknown input data.
		% bias vs variance
		% complexity vs amount of data
		% dimensionality
		% noise
	\item \textbf{Unsupervised}
		learning uses unlabeled training data. It therefore lacks a clear output accuracy metric. Common use cases are clustering and pattern recognition.
	\item \textbf{Semi-supervised}
		learning uses a (generally small) amount of labeled data, supplemented by a comparatively large amount of unlabeled data. Clustering can be used to extrapolate known labels onto unlabeled data points. This is done under the assumption that similar data points share a label.
	\item \textbf{Reinforcement learning}
		relies on a secondary feedback system that evaluates the output accuracy of the model being trained. The feedback system either minimizes a risk function or maximizes a reward function. Reinforcement learning therefore does not require a big dataset of correct and incorrect input-output pairs. Because of the black box behavior of the model, it is not possible to explicitly correct actions that are sub-optimal. As a result, the model can converge on a local minimum.
		% Q-learning
\end{itemize}

\subparagraph{ML category: Goals}\mbox{}\\
ML algorithms can be used for a wide variety of purposes, such as classifying an image or predicting the probability of an event. They are often used for the following goals:
\begin{itemize}
	\item \textbf{Anomaly detection}
		can be divided into 3 categories. \textbf{Supervised anomaly detection} requires data that has a different label for abnormal data, on which it trains a classifier. \textbf{Unsupervised anomaly detection} assumes that normal and abnormal data points are separable, and clusters them. \textbf{Semi-supervised anomaly detection} builds a model of normal data, and tests whether new data points are normal or abnormal. Which of these categories is best depends on dataset characteristics.
	\item \textbf{Classification}
		is the problem of categorizing unknown data points into categories seen during training. This is an inherently supervised process; its unsupervised equivalent is clustering.
	\item \textbf{Clustering}
		is the problem of grouping together data points that are similar according to some metric. This can be solved through supervised learning, but is usually done with large datasets, for which labeling might be expensive.
	\item \textbf{Dimensional reduction}
		is the problem of reducing the number of variables in the input data. This can either be achieved by selecting only relevant variables (\textbf{Feature selection}), or by creating new variables that represent multiple others (\textbf{Feature extraction}).
	\item \textbf{Feature learning / Representation learning}
		is the problem of finding proper representations of input data for e.g. feature detection, classification, clustering, encoding, or matrix factorization.
		% manifold learning
		% statistical inference/density estimation
		% sparse coding
	\item \textbf{Regression}
		is the problem of estimating how dependent variables relate to their dependencies. Parametric regression attempts to encode this in parameters of some function, which speeds up the process if, for example, a linear correlation is expected. Nonparametric regression constructs a predictor according to information derived from the data. It does this without relying on a predetermined form. This requires a larger sample size and significantly more time.
\end{itemize}

\subparagraph{ML category: Types}\mbox{}\\
Every effective ML algorithm needs some kind of "structure" that allows the algorithm to change its parameters so that it can learn new things. Many ML algorithms have the same type, but differ on smaller elements. In this section 4 commonly used overarching types of ML algorithms are presented, namely:
\begin{itemize}
	\item \textbf{Evolutionary algorithms (EAs)} (and 
		specifically \textbf{Genetic algorithms}) learn iteratively based on evolution. The algorithm that actually solves the problem is represented by a set of properties, called its \textbf{genotype}. The performance of the algorithm is measured using a score, calculated using a \textbf{fitness function}. After calculating the fitness score of all generated algorithms, the next iteration creates new genotypes based on mutation and crossover of algorithms that are produce more accurate estimates. Generic algorithms can be used to create other algorithms, such as neural networks, belief networks, decision trees, and rule sets.
	% Graphical models
	\item \textbf{Perceptron-based}
		algorithms use binary classifiers (Perceptrons) that label input vectors as 'active' or 'inactive'. Perceptrons assign a weight to all inputs. It then sum over the products of these weights and their input. The outcome of this is compared to a threshold (known as the \textbf{bias}) to determine the label. Perceptron-based algorithms commonly use the entire batch of training data to try to find an instance that will work for the entire set. They are binary, and therefore primarily used for binary or multi-label classification.
	\item \textbf{Rule-based machine learning (RBML)}
		algorithms use a set of rules that each represent a small part of the problem. These rules usually express a condition, as well as a value for when that condition is met. Because of the clear if-then relation, rules lend themselves to simple interpretation compared to more abstract types of ML algorithms, such as neural networks.
	\item \textbf{Topic Models (TM)}
		are statistical models for finding and mapping semantic structures in text.
\end{itemize}

\subparagraph{ML category: Methods}\mbox{}\\
In this section, an overview of commonly used methods to perform the Machine Learning is presented. These can be considered as children of the Machine Learning types as listed in the section above.
\begin{itemize}
	\item \textbf{Association rule learning}
		is a \textit{rule-based machine learning} method that focuses on finding relations between different variables in datasets. Example relatedness metrics are \textbf{Support} (how often variables appear together), \textbf{Confidence} (how often a causal rule is true) and \textbf{Collective Strength} (inverse likelihood of the current data distribution if a given rule does not exist).
	\item \textbf{Artificial neural networks (ANNs)}
		are perceptron-based systems that consist of multiple layers. These layers are usually divided into an input layer, an output layer, and one or more hidden layers. Each layer consists of nodes connected to the previous and next layers through edges with associated weights (usually called synapses). Unlike "normal" perceptrons, these nodes usually apply an activation function on the output to allow for non-linearities.\\
		The algorithm is defined by the state of the entire network, and can be changed by altering (1) the weights of the synapses, (2) the layout of the network, or (3) the activation function of nodes.\\
		Because neural networks require a large number of nodes, the understandability of a neural network's "thought process" is lower compared to e.g. decision trees.\\
		Neural networks are trained in many different ways, such as by using genetic algorithms. The most common approach, especially when talking about Deep neural networks, is using a process called \textbf{backpropagation};
		\begin{enumerate}
			\item Present a training sample
			\item Calculate the error in each output neuron (the difference between expected and actual output).
			\item Calculate the gradient
			\item For every layer, adjust the weights and biases based on the gradient
			\item Repeat
		\end{enumerate}
		Neural networks are extensively studied because of their ability to analyze enormous sets of data. They can be categorized into several subgroups based on network layout:
		\begin{itemize}
			\item \textbf{Recurrent neural networks (RNNs)}
				keep track of a temporal state in addition to weights, which means that previous inputs of the network influence its current decisions. Neural networks that are not recurrent are called \textbf{feed-forward networks}. Recurrent synapses give the network a "memory". This can help with discovering temporal patterns in data. Blocks of nodes in recurrent network operate as cells with distinct memories, and can information for an arbitrarily long timespan.
				% finite impulse
				% 	DAG which can become a feedforward network 
				% vs infinite impulse
				% 	DAG which can not be unrolled
			\item \textbf{Hopfield networks}
				are a type of non-reflexive, symmetric recurrent neural network that have an "energy" related to every state of the network as a whole. They are guaranteed to converge on a local minimum after some number of network updates.
			\item \textbf{Deep neural networks (DNNs)},
				are the opposite of \textbf{shallow neural networks}: they have many hidden layers. This may cause the network to work as a black-box
				capable of simulating an arbitrary non-linear formula.
			\item \textbf{Convolutional neural networks (CNNs / ConvNets)}
				are deep, feed-forward neural networks that use convolution layers with nodes connected to only a few nodes in the previous layer. These values are then pooled using pooling layers. It can be seen as a way of recognizing abstract features in the data. The convolution makes the network consider only local data. This makes the represented algorithms spatially invariant, which is why they are sometimes called Space Invariant Artificial Neural Networks (SIANN). Chaining multiple of these convolution and pooling layers together can make the network capable of recognizing complicated constructs in big datasets. Examples of this are cats in images or the contextual meaning of a sentence in a paragraph.
			\item \textbf{Generative adversarial networks (GANs)}
				consist of two separate networks: one trying to recognize objects from a dataset, and one trying to create new data in an attempt to 'fool' the other network into thinking the data is legit.\cite{Li:2013:CAL:2463372.2465801}
			\item \textbf{Radial Basis Function (RBF) networks}
				\cite{rbf}
			\item \textbf{Self-organizing maps (SOMs) / self-organizing feature maps (SOFMs)}
				are neural networks that learn through unsupervised \textbf{competitive learning}, in which nodes compete for access to specific inputs. This causes the nodes to become highly specialized, which reduces redundancy. The iterations effectively move the map closer to the training data, which is the reason for its name. Some subtypes include the Time Adaptive Self-Organizing Map (TASOM), Binary Tree TASOM (BTASOM) and Growing Self-Organizing map (GSOM)
			\item \textbf{Stochastic neural networks}
				make use of stochastic transfer functions or weights, which allows them to escape local minima. 
			% 		boltzmann machine
		\end{itemize}
	\item \textbf{Auto-encoders}
		are a type of neural network that trained specifically to encode and decode data. Because auto-encoders are trained to perform decoding separately from encoding, the encoded version of the data can be seen as a dimensional reduction of the data.
	\item \textbf{Bayesian networks}, 
		sometimes called \textbf{belief networks}, are probabilistic directed acyclic graphical models\cite{Wain08}\cite{Kol09}\cite{Xin16} that are used to represent conditional relationships between variables. They are directed, so that they can overcome the problems of \textbf{Markov random
		fields (MRFs)} (also called \textbf{Markov networks}), which use undirected connections. These undirected connections make it impossible to represent dependencies that are non-transitive or otherwise induced. Bayesian networks are commonly used to represent \textbf{Markov processes} (not related to Markov networks), in particular for probabilistic inference or parameter estimations.
		\textbf{Nonparametric Bayesian models}\cite{Grif05}\cite{Teh06} do not fix the parameters in place, allowing the model to grow with the data size.
	%		Regularized Bayesian models \cite{Zhu09}\cite{Zhu09-2}\cite{Zhu14}
	\item \textbf{Decision trees},
		sometimes called CART trees, after Classification And Regression Trees (RAS syndrome), use rule-based machine learning to create a set of rules and decision branches. Traversing the tree involves applying the rules at each step until a leaf of the tree is reached. This leaf represents the decision or classification for that input.
	\item \textbf{Latent Dirichlet Allocation}\cite{Blei03}
		constructs a mapping between documents and a probabilistic set of topics, using the assumption that documents have few different topics and that those topics use few different words.
	\item \textbf{Latent semantic analysis (LSA) / latent semantic indexing (LSI)}
		creates a big matrix of documents and topics in an attempt to classify documents or to find relations between topics. LSA/LSI assumes a Gaussian distribution for topics and documents. LSA/LSI does not have a way of dealing with words that have multiple meanings.
	\item \textbf{Naive Bayes classifiers}
		are relatively simple probabilistic classifiers that assume different features to be independent. They can be trained quickly using supervised learning, but are less accurate than more complicated approaches.
	\item \textbf{Probabilistic latent semantic analysis (PLSA) / probabilistic latent semantic indexing (PLSI)}
		is the same as LSA/LSI except that PLSA/PLSI assumes a Poisson distribution for topics and documents instead of the Gaussian distribution that is assumed by LSA/LSI. The reason is that a Poisson distribution appears to model the real world better. Some subtypes include Multinomial Asymmetric Hierarchical Analysis (MASHA), Hierarchical Probabilistic Latent Semantic Analysis (HPLSA), and Latent Dirichlet Allocation (LDA).
	% Inductive logic programming (ILP)
	\item \textbf{Support vector machines (SVMs)}
		map data points to high dimensional vectors for classification and clustering purposes. For data points in a p-dimensional space, a (p-1)-dimensional hyperplane can be used as a classifier. A reasonable choice would be the hyperplane that properly separates the data points in two groups based on their labels by the largest possible margin. Sometimes special transformation equations (called \textbf{kernels}) are used to transform all data points to a different representation, in which it is easier to find such a hyperplane.\\
		Having a lot of dimensions decreases the accuracy of classifying new data points, so increasingly large datasets are needed to make the algorithms perform well.
\end{itemize}

\subparagraph{Combining ML algorithms: Ensemble Methods}\mbox{}\\
Usually, a single algorithm is not accurate enough to solve a problem. Because of that, multiple algorithms are sometimes combined in so-called \textbf{Ensemble Learning}. There are many different ways to do this, such as:

\begin{itemize}
	\item \textbf{Bagging}
		is the process of building multiple classifiers and combining them into one.
	\item \textbf{Boosting}
		is the process of training new models with the data that is misclassified by the previous models.
	\item \textbf{Bucketing}
		is the process of training many different models and eventually selecting the one that has the best performance.
	\item \textbf{Random Forests}\cite{Breiman2001}
		use multiple decision trees and averaging the prediction made by the individual trees as to increase the overall accuracy. Different trees are given the same 'voting power'.
	\item \textbf{Stacking}
		is when multiple classifiers are trained on the dataset, and one new classifier uses the output of the other classifiers as input in an attempt to reduce the variance.
	\item \textbf{Learning Classifier Systems (LCSs)}
		(not to be confused with the more common usage of the LCS initialism, Longest Common Subsequence) is a modular system of learning approaches. An LCS iterates over data points from the dataset, completing the entire learning process in each iteration. The main idea is that an LCS has a limited number of rules. A Genetic Algorithm forces suboptimal rules out of the rule set. There are many different attributes that can drastically change the performance of an LCS depending on the dataset, including the Michigan-style vs Pittsburgh-style architecture \cite{Puig09}, supervised vs reinforcement learning \cite{kaelbling1996reinforcement}, incremental vs batch learning \cite{clearwater1989incremental}, online vs offline training, strength-based vs accuracy-based \cite{wilson1995classifier} and complete mapping vs best mapping.\\		
		Because of the increasing complexity of training an LCS with respect to its data size, it is the most commonly used implementation.
\end{itemize}


%Deep learning
%
%Multi-task learning (MTL) 
%
%Matrix factorization
%Singular value decomposition (SVD) and principal component analysis (PCA)


\paragraph{Parameter optimization}
To find the best parameters for these algorithms, several Machine Learning workhorse implementations can be used that can be re-used across different Machine Learning algorithm-families. These include:
\begin{itemize}
	\item First-order techniques such as stochastic gradient descent (SGD)\cite{bottou2010large}, stochastic dual coordinate ascent\cite{Shal13}, L-BFGS\cite{liu1989limited} or conjugate gradient methods\cite{hestenes1952methods}\cite{dai1999nonlinear}
	\item Second-order techniques such as Newton descent\cite{polyak2007newton} (which requires computing the Hessian, and is therefore generally infeasible) or Quasi-Newton descent\cite{byrd2016stochastic} (which approximates Newton descent)
	\item Coordinate descent\cite{wright2015coordinate}, which minimizes along coordinate directions
	\item Markov-Chain Monte-Carlo\cite{brooks1998markov}, which samples from a probability distribution
	\item Variational inference\cite{Blei17}, which approximates probability densities through optimization
\end{itemize}


% --------


\subsubsection{Partitioning and distribution algorithms}
To execute the ML algorithms listed in the previous section, of which the parameters are optimized by ML workhorses discussed (such as first-order / second-order techniques), we can either execute them on a single computer (which is out of the scope of this paper) or in a distributed manner. There are several ways to partition the data and/or the program and to distribute these evenly across all machines.


\paragraph{Computation time vs communication vs accuracy}

When Distributed Machine Learning is used on a dataset small enough to use with regular Machine Learning, parallelizing the learning can reduce computation time, as long as the communication costs are not too high. This can become a problem if the model being trained is sufficiently large in comparison to the data. Splitting up the dataset across different machines reduces the accuracy of the individual machines, but depending on the combination of the learning models, the overall accuracy can be better or worse than what a single machine would come up with.

In data that is distributed already, more communication increases accuracy, but big models can very quickly cause the communication costs to be much higher than the actual computation time. It usually comes down to seeking the amount of communication needed to achieve the desired accuracy.


\paragraph{Bridging computation and communication}
To schedule and balance the workload, there are 3 things that have to be taken into account:\cite{Xing16}\\
\begin{itemize}
	\item Deciding which tasks can execute in parallel
	\item Deciding the order in which tasks will be executed
	\item Ensuring an even load distribution across the available machines
\end{itemize}
After having made the above-mentioned 3 decisions, the information between nodes should be communicated as efficiently as possible. There are several techniques that take the interleaving of parallel program computations and inter-worker communication into account. These techniques trade off fast / correct model convergence (at the top of the list) with faster / fresher updates (at the bottom of the list).
\begin{itemize}
	\item \textbf{Bulk Synchronous Parallel (BSP)} is the most simple model, in which programs will alternate between a computation and a communication phase to ensure consistency\cite{Xing16}. An example of program following the BSP bridging model is MapReduce.\\
	An advantage is that serializable BSP ML programs are guaranteed to output a correct solution. A disadvantage is that finished workers must wait at every synchronization barrier until the others are finished, which results in overhead\cite{Chilimbi14}. Another disadvantage is that the synchronization barrier may take a significant amount of time, because of slow communication between the workers.
	\item \textbf{Stale Synchronous Parallel (SSP)} allows the fastest worker to be ahead of the slowest worker up to a bounded number of iterations. If this number is exceeded, all workers are paused. That way, fast workers may continue for a while on stale data. However, SSP still limits the maximum staleness between any pair of workers. An advantage is that it still enjoys strong model convergence guarantees. A disadvantage is that, when machines temporarily slow down due to other tasks or users and cause large staleness, the convergence rates are poor.
	\item \textbf{Approximate Synchronous Parallel (ASP)} limits how inaccurate a parameter can be. This contrasts with SSP, which limits how stale a parameter can be. An advantage is that, whenever an aggregated update is insignificant, the server can delay synchronization indefinitely. A disadvantage is that it can be hard to choose the parameter that defines which update are significant and which are not. \cite{Hsieh17}
	\item \textbf{Barrierless Asynchronous Parallel\cite{Han15} / Total Asynchronous Parallel\cite{Hsieh17} (BAP / TAP)} lets worker machines communicate in parallel without waiting for each other. The advantage is that it usually obtains the highest possible speedup. A disadvantage is that the model can converge slowly or even incorrectly because, unlike BSP and SSP, the error grows with the delay. \cite{Han15}
\end{itemize}
At the moment, ASP is the state-of-the-art method.


\paragraph{Communication strategies}
To distribute Machine Learning algorithms, either the data or the model is partitioned across the machines. This is referred to respectively as data parallelism and model parallelism \cite{Die12}. These two methods can also be applied simultaneously\cite{Xing16}. Data parallelism partitions the data and assigns it to parallel machines. It can be used with every ML algorithm with an i.i.d. assumption over the data samples (i.e. most ML algorithms \cite{Xing16}). Model parallelism partitions the model parameters and assigns that to parallel machines. It cannot be applied to most ML algorithms, because the model parameters generally do not satisfy this i.i.d. assumption.\\
There are several communication management strategies\cite{Xing16} used to spread and reduce the amount of data communicated between machines:
\begin{itemize}
	\item To prevent bursts of communication over the network (e.g. after a mapper is finished), continuous communication is used, such as in the state-of-the-art implementation B\"osen\cite{Wei15}.
	
	\item Neural networks are composed out of layers, the training of which (using the back-propagation gradient descent algorithm) is highly sequential. Because the top layers of neural networks contain the most parameters while accounting only for a small part of the total computation \cite{Xing16}, Wait-free Backpropagation (WFBP) \cite{Zhang17} was proposed to spread the computation and communication out in an optimal fashion.
	
	\item Because WFBP does not reduce the communication overhead, hybrid communication (HybComm) \cite{Zhang17} was proposed. Effectively, it combines Parameter Servers (PS)\cite{Wei15} with Sufficient Factor Broadcasting (SFB)\cite{Xie15} by being aware of both the mathematical property of neural networks and the structure of computing clusters. See below for more information about PS (under Centralized Storage) and SFB (under Decentralized Storage).
	
\end{itemize}


\paragraph{Network topologies}
The last consideration for the design of a Distributed Machine Learning cluster is the way the computers within the cluster are organized. There are several network topologies used for Distributed Machine Learning clusters:
\begin{itemize}
	\item \textbf{Trees} AllReduce\cite{Agar14} is an example of a tree-like network topology that provides straightforward parallelization of gradient-based optimization algorithms. It achieves this by accumulating local gradients to obtain a global gradient.\\
	An advantage is that AllReduce is very fast and highly scalable. A disadvantage is that it only works for reduce operations (like in MapReduce).\\
	\begin{minipage}{\linewidth}
		\centering
		\includegraphics[scale=0.5]{AllReduce.png}
		\captionof{figure}{AllReduce operation. Initially, each node holds its own value. Values are passed up the tree and summed, until the global sum is obtained in the root node (reduce phase). The global sum is then passed back down to all other nodes (broadcast	phase). At the end, each node contains the global sum.}
	\end{minipage}
	\item \textbf{Centralized storage} Centralized storage is used for topologies where a single "master" or "server" node is in the center, and many "slave" or "client" nodes communicate only with this master node. A practical implementation of this is the Parameter Server paradigm (PS). Each Parameter Server keeps a shard of the global model parameters as a key-value store. Each client communicates with the Parameter Server to read / update the parameters. \\
	An advantage is that all model parameters (within a shard) are in a global shared memory, which makes it easy to inspect the model. A disadvantage is that the Parameter Servers can form a bottleneck, because they are handling all communication. To partially alleviate this issue, the techniques mentioned under "Bridging computation and communication" are used.
	\item \textbf{Decentralized storage} In contrast to centralized storage, in decentralized storage every worker maintains its own local view of the parameters, and the workers communicate directly with each other. An example implementation is a peer-to-peer network where every machine broadcasts everything to all other machines. To reduce the amount of communication between all workers, Sufficient Factor Broadcasting (SFB)\cite{Li13}) was proposed. It decomposes the parameter matrix into so-called sufficient factors, i.e. 2 vectors that are sufficient to reconstruct the update matrix. SFB only broadcasts the sufficient factors and lets the workers reconstruct the updates.\\
	An advantage is that, with SFB, decentralized storage is relatively communication-efficient. In addition, there is no centralized bottleneck, making it more scalable.
\end{itemize}










\subsection{Currently used implementations}
This section first shows some generic implementations for distributed systems. These are often used as the foundation for distributed Machine Learning systems. Additionally, several domain-specific implementations are available. Of these, the most popular ones are described, including Distributed Ensemble Learning, Parallel Synchronous Stochastic Gradient Descent (SGD), and Parameter Servers.

\input{./Generic_implementations.tex}
\input{./sections/domain_specific_implementations.tex}
\subsubsection{Current challenges}
\paragraph{Performance}

A trade-off that’s seen frequently is the reduction of wall-clock time at the expense of total processing time (i.e. decreased efficiency). When compute resources are affordable enough, many real-world use cases of machine learning benefit most from being trained rapidly. The fact that this often implies a large multiple in total compute resources (and the associated energy consumption) is not as important, as long as a model saves more money than it costs to run.  A good example of this is found in \citet{DistBelief2012}, where wall clock time speedup factors are achieved by increasing the number of machines quadratically or worse. It still delivered Google competitive advantage for years. Distributed use of GPUs, as in Tensorflow, has better properties, but often still exhibits efficiency below 75\%.
These performance concerns are much less severe in the context of synchronous SGD-based frameworks, which often do achieve linear speedups in benchmarks. However, most of these benchmarks test at most a few hundred machines, whereas the scale at which e.g. DistBelief is demonstrated can sometimes be two orders of magnitude larger. The reason for this is unclear, although it might be related to the fault tolerance concerns mentioned below. Synchronous frameworks also benefit more from high-bandwidth links such as InfiniBand, due to the fact that all nodes' communication is synchronized. However, those are expensive (with costs running up to thousands of euros for even a cheap switch) and cannot practically be used in large footprint clusters (e.g. in multiple datacenters) due to wiring and latency constraints.

\paragraph{Fault tolerance}

Synchronous AllReduce-based approaches seem to scale significantly better than the parameter server approach (up to a certain cluster size), but suffer from a lack of fault-tolerance: failure of a single machine blocks the entire training process. At smaller scales, this might still be a manageable problem. However, past a certain number of nodes the probability of any node being unavailable becomes high enough to result in near-continuous stalling. Common implementations of these High-Performance Computing-inspired patterns, such as MPI and NCCL, lack fault-tolerance completely. Although there are efforts to counteract some of this, production-ready solutions are lacking. Some of the described implementations allow for checkpointing to counteract this, but a lot of work is necessary to enable true fault-tolerance, as is described in \citet{Amatya2017}. It is also possible to reduce the probability of failure for each individual node, but this requires very specific hardware that is expensive (e.g. highly stable datacenter cooling or Infiniband networks) and not generally available on commodity cloud platforms.
Asynchronous implementations do not suffer from this problem as much. They are designed to explicitly tolerate straggling and failing nodes, with only minimal impact on training performance. The question for ML practitioners, then, is whether they prefer performance or fault tolerance, and whether they are constrained by either one. Hybrid approaches even offer a way to customize these characteristics, although they are not frequently found in use yet. It would be interesting to see whether an even better approach exists, or whether there is an efficient way to implement fault-tolerant AllReduce.

\paragraph{Privacy}

There are scenarios in which it is beneficial or even mandatory to isolate different subsets of the training data from each other. The furthest extent of this is when a model needs to be trained on datasets that each live on different machines/clusters, and may under no circumstance be co-located or even moved.
One interesting approach to training models in a privacy-sensitive context is the use of a distributed ensemble model. This allows perfect separation of the training data subsets, with the drawback that a method needs to be found that properly balances each trained model's output for an unbiased result.
Parameter server-based systems could also be useful in the context of privacy, as the training of a model can be separated from the training's result. However, this assumes that no sensitive properties of the underlying data leak into the model itself, which is not always easy to prove.
Finally, it is possible to introduce statistical noise into each subset of the training data, with the intention of rendering its sensitive characteristics unidentifiable to other parties. \citet{Bal12} touches on this subject, but makes it clear that the amount privacy in this scenario is dependent on the amount of statistical queries required to learn the dataset. This puts an upper bound on usefulness of the model itself.
In conclusion, it is highly unlikely that perfect privacy is possible, but current frameworks do not offer much support for even basic variants. It could be interesting to answer whether there is a generic way to facilitate distributed privacy, which could then be integrated into the frameworks that are being used.

