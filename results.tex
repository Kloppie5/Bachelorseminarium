\section{Results}
% Answer research questions

\subsection{The need for Distributed Machine Learning}
\subsubsection{Alternatives to Distributed Machine Learning}
\paragraph{Long-term sustainability}










\subsection{Underlying technology}
To give you an overview of how Distributed Machine Learning works, we'll give you an abstract framework that includes everything a real implementation should include. We do that by exploiting the three unique properties of Distributed Machine Learning, namely error tolerance, dynamic structural dependency and non-uniform convergence.\cite{Xing16}\\
Our goals include (1) to list regular Machine Learning algorithms that are commonly used in a distributed setting; (2) to find algorithms to determine the best parameters for the former algorithms; (3) to trade-off computation time with communication and accuracy; and (4) to minimize the amount of bits sent over the network so that the system is no bottlenecked by scarce network bandwidth.\\
Designing a general system in such a way that the regular Machine Learning algorithms can be distributed efficiently is challenging, because every algorithm has its own communication patterns \cite{Jia14}\cite{Newman09}\cite{Rich13}\cite{Smola10}\cite{Takac13}\cite{Tsi12}.

\subsubsection{Machine Learning algorithms}
We'll first look at Machine Learning is done at a single machine before looking at distributing these. We'll only give a brief description of the algorithms because single-node machine learning is not the focus of this paper.


\paragraph{Overview}
Machine Learning algorithm-families that are used in the literature and perform the actual "learning" include:
\begin{itemize}
	\item Graphical models \cite{Wain08}\cite{Kol09}\cite{Xin16}
	\begin{itemize}
		\item Latent Dirichlet Allocation Topic Model \cite{Blei03}
	\end{itemize}
	\item Regularized Bayesian models \cite{Zhu09}\cite{Zhu09-2}\cite{Zhu14}
	\item Nonparametric Bayesian models \cite{Grif05}\cite{Teh06}
	\item Sparse structured models
	\begin{itemize}
		\item Lasso regression (group Lasso)
	\end{itemize}
	\item Large-margin methods
	\item Deep learning
	\item Matrix factorization
	\item Sparse coding
	\item Latent space modeling
	\item Distance metric learning
	\item Non-negative matrix factorization
	\item Principal component analysis
\end{itemize}


\paragraph{Algorithms to find best parameters}
To find the parameters for these algorithms we can use several Machine Learning workhorse implementations that can be re-used across different Machine Learning algorithm-families. These include:
\begin{itemize}
	\item First-order techniques
	\begin{itemize}
		\item Stochastic gradient descent
		\item Stochastic dual coordinate ascent\cite{Shal13}
		\item L-BFGS
		\item Conjugate gradient
	\end{itemize}
	\item Second-order techniques
	\begin{itemize}
		\item Newton descent
		\item Quasi-Newton descent
	\end{itemize}
	\item Coordinate descent
	\item Markov-Chain Monte-Carlo
	\item Variational inference
\end{itemize}


% --------


\subsubsection{Partitioning and distribution algorithms}
Now we'll look the abstraction of the general design for a distributed Machine Learning operating system that executes the Machine Learning workhorses across a wide variety of hardware.


\paragraph{Computation time vs communication vs accuracy}


\paragraph{Scheduling and balancing the workloads}
There are 3 things to take into account when partitioning an ML program in order to parallelize it:\cite{Xing16}\\
\begin{enumerate}
	\item Deciding which tasks go or don' go together in parallel
	\item Deciding the order in which tasks will be executed
	\item Ensuring an even load distribution across the machines
\end{enumerate}


\paragraph{Bridging computation and communication}
Let's look at several ways to efficiently communicate between the nodes taking the interleaving of parallel program computations and inter-worker communication into account.
\begin{enumerate}
	\item \underline{BSP:} Bulk Synchronous Parallel, the most simple model; programs will alternate between a computation and a communication phase to ensure consistency\cite{Xing16}. An example of program following the BSP bridging model is MapReduce.\\
	An advantage is that serializable BSP ML programs are guaranteed to output a correct solution. A disadvantage is that finished workers must wait at every synchronization barrier till the other works are finished, which results in overhead\cite{Chilimbi14}. Another disadvantage is that the synchronization barrier may take a significant amount of time, because of slow communication between the workers.
	\item \underline{SSP:} Stale Synchronous Parallel, allows the fastest worker to be ahead of the slowest worker up to a bounded number of iterations before the workers are stopped. The fast workers may work for a while on state data, but SSP still limits the maximum staleness between any pair of workers. An advantage is that it still enjoys strong model convergence guarantees. A disadvantage is that, when machines temporarily slow down due to other tasks or users and cause large staleness, the convergence rates are poor.
	\item \underline{ASP:} Approximate Synchronous Parallel, limits how inaccurate a parameter can be, in contrast to SSP that limits how stale a parameter can be. An advantage is that, when an aggregated update is insignificant, then the server can delay synchronization indefinitely. A disadvantage is that it can be hard to choose the parameter that defines which update are significant and which are not. \cite{Hsieh17}
	\item \underline{BAP / TAP:} Barrierless Asynchronous Parallel\cite{Han15} or Total Asynchronous Parallel\cite{Hsieh17}; worker machines communicate in parallel without waiting for each other. The advantage is that it usually obtains the highest speedup possible. A disadvantage is that the model converges slowly or even incorrectly because the error grows with the delay, unlike BSP and SSP. \cite{Han15}
\end{enumerate}
At the moment, ASP is the state-of-the-art method.


\paragraph{Communication strategies}
To distribute Machine Learning algorithms we can choose to partition either the data or the model across the machines - referred to respectively as data parallelism and model parallelism \cite{Die12}. These two types of parallelism can be applied at the same time \cite{Xing16}. Data parallelism can be applied to every ML algorithm with an i.i.d. assumption over the data samples, which are most ML algorithms \cite{Xing16}. It partitions the data and assigns it to parallel machines. Model parallelism cannot be applied to most ML algorithms, because the model parameters generally don't have this i.i.d. assumption. It partitions the model parameters and assigns that to parallel machines.\\
There are several communication management strategies\cite{Xing16} used to spread and reduce the amount of data communicated between machines:
\begin{itemize}
	\item To prevent bursts of communication over the network, for example after a mapper is finished, continuous communication is used, for example in the state-of-the-art implementation Bösen\cite{Wei15}.
	
	\item Neural networks are composed out of layers, of which the training by using the back-propagation gradient descent algorithm is highly sequential. Because the top layers of neural networks contain a lot more parameters while it accounts only for a small part of the total computation \cite{Xing16}, WFBP \cite{Zhang17} was proposed to spread the computation and communication out in an optimal fashion.
	
	\item Hybrid communication (HybComm) Because WBFP does not reduce the communication overhead, HybComm \cite{Zhang17} was proposed. Effectively it combines Parameter Servers (PS)\cite{Wei15} with Sufficient Factor Broadcasting (SFB)\cite{Xie15} by being aware of both the mathematical property of neural networks and the structure of computing clusters. See below for more information about PS and SFB.
	
\end{itemize}


\paragraph{Network topologies}
There are several network topologies used for Distributed Machine Learning clusters:
\begin{itemize}
	\item \textbf{Trees} AllReduce\cite{Agar14}
	\item \textbf{Centralized storage} Parameter Server (PS)\cite{Agar14}
	\item \textbf{Decentralized storage} e.g. peer-to-peer storage (e.g. Sufficient Factor Broadcasting (SFB)\cite{Li13})
\end{itemize}










\subsection{Currently used implementations}
\subsubsection{Generic distributed system frameworks}
\input{./sections/domain_specific_implementations.tex}

\subsubsection{Current challenges}

\paragraph{Performance}

A tradeoff that’s seen frequently is the reduction of wall-clock time at the expense of total processing time (i.e. decreased efficiency). When compute resources are affordable enough, many real-world use cases of machine learning benefit most from being trained rapidly. The fact that this often implies a large multiple in total compute resources (and the associated energy consumption) is not as important, as long as a model saves more money than it costs to run.  A good example of this is found in \citet{DistBelief2012}, where wall clock time speedup factors are achieved by increasing the number of machines quadratically or worse. It still delivered Google competitive advantage for years. Distributed use of GPUs, as in Tensorflow, has better properties, but often still exhibits efficiency below 75\%.
These performance concerns are much less severe in the context of synchronous SGD-based frameworks, which often do achieve linear speedups in benchmarks. However, most of these benchmarks test at most a few hundred machines, whereas the scale at which e.g. DistBelief is demonstrated can sometimes be two orders of magnitude larger. The reason for this is unclear, although it might be related to the fault tolerance concerns mentioned below. Synchronous frameworks also benefit more from high-bandwidth links such as InfiniBand due to the fact that all nodes' communication is synchronized, even though they are expensive (with costs running up to thousands of euros for even a cheap switch) and can't practically be used in large footprint clusters (e.g. in multiple datacenters) due to wiring and latency constraints.

\paragraph{Fault tolerance}

Synchronous AllReduce-based approaches seem to scale significantly better than the parameter server approach (up to a certain cluster size), but suffer from a lack of fault-tolerance: failure of a single machine blocks the entire training process. At smaller scales this might still be a manageable problem, but past a certain number of nodes the probability of any node being unavailable becomes high enough to lead to near-continuous stalling. Common implementations of these HPC-inspired patterns, such as MPI and NCCL, lack fault-tolerance completely, and although there are efforts to counteract some of this, . Some of the described implementations allow for checkpointing to counteract this, but a lot of work is necessary to enable true fault-tolerance, such as described in \citet{Amatya2017}. It is also possible to reduce the probability of failure for each individual node, but this requires very specific hardware that is expensive (e.g. Infiniband) and not generally available on commodity cloud platforms.
Asynchronous implementations do not suffer from this problem quite as much, as they are designed to explicitly tolerate straggling and failing nodes, with only minimal impact on training performance. The question for ML practitioners, then, is whether they prefer performance or fault tolerance, and whether they are constrained by either one. Hybrid approaches even offer a way to customize these characteristics, although they are not frequently found in use yet. It would be interesting to see whether an even better approach exists, or whether there is an efficient way to implement fault-tolerant AllReduce.

\paragraph{Privacy}

There are scenarios in which it is beneficial or even mandatory to isolate different subsets of the training data from each other. The furthest extent of this is when a model needs to be trained on datasets that each live on different machines/clusters, and may under no circumstance be colocated or even moved.
One interesting approach to training a model in a privacy-sensitive context is the use of a distributed ensemble model. This allows perfect separation of the training data subsets, with the drawback that a method needs to be found to properly balance each trained model's output for an unbiased result.
Parameter server-based systems could also be useful in the context of privacy, as the training of a model can be separated from the training's result. However, this assumes that no sensitive properties of the underlying data leak into the model itself, which is not easy to prove.
Finally, it is possible to introduce statistical noise into each subset of the training data, with the intention of rendering its sensitive characteristics unidentifiable to other parties. \citet{Bal12} touches on this subject, but makes it clear that the amount privacy in this scenario is dependent on the amount of statistical queries required to learn the dataset, which puts an upper bound on the usefulness of the model itself.
In conclusion, it is highly unlikely that perfect privacy is possible, but current frameworks do not offer much support for even basic variants. It could be interesting to answer whether there's a generic way to facilitate distributed privacy, which could then be integrated into the frameworks that are being used.

