\section{Results}
% Answer research questions

\subsection{The need for Distributed Machine Learning}
\subsubsection{Alternatives to Distributed Machine Learning}
\paragraph{Long-term sustainability}










\subsection{Underlying technology}
To give you an overview of how Distributed Machine Learning works, we'll give you an abstract framework that includes everything a real implementation should include. We do that by exploiting the three unique properties of Distributed Machine Learning, namely error tolerance, dynamic structural dependency and non-uniform convergence.\citet{Xing16}\\
First we'll look at regular Machine Learning algorithms that can be adapted so that they can be used for Distributed Machine Learning.\\
Then we'll look at the algorithms that are used to partition and distribute these algorithms to that they can be used across a wide variety of hardware.
\subsubsection{Machine Learning algorithms}
\paragraph{Overview}
\paragraph{Algorithms to find best parameters}
\subsubsection{Partitioning and distribution algorithms}
\paragraph{Computation time vs communication vs accuracy}
\paragraph{Scheduling and balancing the workloads}
\paragraph{Bridging computation and communication}
\paragraph{How to communicate?}
\paragraph{What to communicate?}










\subsection{Currently used implementations}
\subsubsection{Generic distributed system frameworks}
\input{./sections/domain_specific_implementations.tex}

\subsubsection{Current challenges}

\paragraph{Performance}

A tradeoff that’s seen frequently is the reduction of wall-clock time at cost of significantly increased total processing time. When compute resources are affordable enough, many real-world use cases of machine learning benefit most from being trained rapidly. The fact that this often implies a large multiple in total compute resources (and the associated energy consumption) is not as important, given the revenues and cost-reductions that applying machine learning can effect.  A good example of this is the DistBelief paper, where wall clock time speedup factors are achieved by increasing the number of machines quadratically or worse, and data parallelism only comes out ahead of GPUs when using more than 20 machines. Distributed use of GPUs, as in Tensorflow, has better properties, 
MPI-based approaches also suffer less from this, borrowing some properties from the HPC setting. The drawback to this is that HPC is not quite as concerned with fault tolerance, as it often depends on expensive hardware that simply doesn’t fail as often (think Infiniband, storage area networks, etc.) - extra scalability is not bought cheaply, and with ever-diminishing returns. Big data often reaches a scale at which this is simply no longer economical. At smaller scales, it might be the best solution for the moment, but it would be interesting to see what industry can come up with that is both easy to use, affordable, scalable, and computation-efficient.

\paragraph{Fault tolerance}

MPI-based approaches seem to scale significantly better than the parameter server approach, but suffer from a lack of fault-tolerance: failure of a single machine blocks the entire training process. At smaller scales this might still be a manageable problem, but past a certain number of nodes the probability of node failure becomes high enough that a lack of fault tolerance is a blocker.

\paragraph{Privacy}

There are scenarios in which it is beneficial or even mandatory to isolate different subsets of the training data from each other. The furthest extent of this is when a model needs to be trained on datasets that each live on different machines/clusters, and may under no circumstance be colocated or even moved.
One interesting approach to training a model in a privacy-sensitive context is the use of a distributed ensemble model.
