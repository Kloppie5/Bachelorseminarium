\section{Methods}
In this section we explain how we aggregated the information from other literature in this overview.

\subsection{Study selection}
To review the current state of the art of Distributed Machine Learning, we used 4 reference papers\cite{Bal12}\cite{Die12}\cite{Xing16}\cite{Zhang16} and used those as starting point to find more relevant literature. We did not necessarily try to limit literature for this paper to the most recent ones, because there are several non-recent papers that are still very relevant, e.g. the paper about MapReduce\cite{Dean04}.
\subsection{Procedure}
After reading the reference papers \cite{Bal12}\cite{Die12}\cite{Xing16}\cite{Zhang16} we made an initial structure for the review and then started looking for relevant literature for every individual (sub-)section.
\subsection{Publication bias}
For this review many studies with smaller effect sizes are missing, because we focused on the papers with the most references since these generally contributed most to the field. We also did not analyze papers that proposed a new method that performed worse than the older ones. While it is important to know what techniques do not work well, in this paper we want to show you the current state-of-the-art, which includes only techniques that are the best techniques currently available.